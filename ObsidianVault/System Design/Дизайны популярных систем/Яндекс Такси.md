#SystemDesign 

## Функциональные требования
- заказ такси пользователем
- начало конец работы водителя
- подбор ближайшего водителя для поездки 
- подтверждение и отклонение заказа водителем
- наблюдение за поездкой
- история поездок

## Нефункциональные требования
- DAU (users) 100 000 000
- DAU (drivers) 5 000 000
- avaliability 99,95%
- eventual consistency
- каждый пользователь ездит 1 раз в день
- средняя продолжительность поездки 30 минут
- каждый водитель отрабатывает 20 заказов в день
- responce time на заказ такси 1 минута
- геораспределенности нет
- сезонности нет

[[yandex-taxi]]

### Как хранить координаты?

Использовать гео-хеш ([[Как работает гео-хеш]]), так как нам нужно будет искать ближайших родителей. Если использовать какие-то древоподобные структуры данных на таком RPS это все будет работать медленно, так как надо будет их перестраивать.

Если в данном гео-хеше нет ни одного водителя, можно сканить соседние гео-хеши(как правило этот момент обговаривается с заказчиком)

Когда мы нашли всех водителей в гео-хеше, мы начинаем ==синхронно== посылать им запросы на подтверждение заказа. Так как по требованиям responce time 1 минута нас это устраивает(~10сек поиск водителей + по 10 сек на ответ от водителя = -+ опросить 5 водителей)

==Matching Service== - cмотрит в каком гео-хеше клиент и находит ему там водителей. Он довольно долго хранит в себе состояние запроса клиента, если он упадет посреди запроса, клиента откинет на этап ввода запроса. Чтобы это исправить можно хранить какие-то состояния в in-memory БД

==Ride Service== - после подтверждения водителем заказа, этот сервис будет отправлять события о статусе поездки(водитель едет, водитель будет через 5 мин, водитель приехал), а так же трекать водителя. На случай падения этого сервиса необходимо сделать in-memory хранилище для кеша поездки.

>[!info]- Возможная проблема!
> Если водитель будет пушить свои координаты через RideService, который будет ходить в GeoService и обновлять эти координаты, то может возникнуть проблема, когда придет новый клиент и сделает заказ, ему отдадут 10 водителей, каждый из которых выполняет заказ. 
> 
> Чтобы решить эту проблему надо сделать две отдельные структуры данных в хранилище GeoService, для водителя выполняющего заказ и для свободного водителя 

==NotificationService== - сервис отвечающий за оповещение водителя и клиента о поездке и ее состоянии. Хранит в себе коннекшены пользователей и их водителей. Предлагает водителю поездку, говорит клиенту о ее подтверждении, держит с водителем связь по web socket. Так как водителей у нас точно будет больше 10 000, то сервис этот надо масштабировать(+ убрать единую точку отказа)

Варианты:
1.  Поднять пул сервисов, запоминать с каким сервисом устанавливает связь клиент 1 раз, далее общаться только с этим сервисом. Когда поездка завершена сервис кидает уведомление о том, что он освободился
2. Использовать Service Discovery. Запрос клиента идет через сервис Service Discovery, который дает ему адрес его NotificationService и запоминает адрес клиента. Когда Marching или Notification сервисы хотят отправить клиенту запрос, он так же идет через Service Discovery

## Отказоустойчивость:
- Load Balancer - несколько инстансов
- Api Gateway - несколько инстансов
- Matching Service - несколько инстансов, если падает 1, то восстанавливает состояние из БД
- Ride Service - несколько инстансов, если падает 1, то восстанавливает состояние из кеша в БД
- History Service - несколько инстансов
- Tarantool для Geo Service - потеря данных не критична, так как они автоматически перенальются в следующую итерацию. Только реплика на случай падения
- Tarantool для Matching Service и Ride Service - очень важно не терять данные, поэтому WAL и репликация
- ClickHouse - реплицируется, по дефолту асинхронная репликация
